{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n100%|█████████▉| 9912320/9912422 [09:23<00:00, 22184.95it/s]Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n\n0it [00:00, ?it/s]\u001b[ADownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00<?, ?it/s]\u001b[A\n 28%|██▊       | 8192/28881 [00:00<00:00, 32145.49it/s]\u001b[A\n32768it [00:01, 31681.50it/s]\n\n0it [00:00, ?it/s]\u001b[AExtracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[A\n  1%|          | 16384/1648877 [00:01<00:50, 32099.46it/s]\u001b[A\n  2%|▏         | 32768/1648877 [00:01<00:43, 37580.16it/s]\u001b[A\n  2%|▏         | 40960/1648877 [00:01<00:44, 36443.36it/s]\u001b[A\n  3%|▎         | 57344/1648877 [00:01<00:38, 41660.58it/s]\u001b[A\n  4%|▍         | 65536/1648877 [00:02<00:40, 39114.06it/s]\u001b[A\n  5%|▍         | 81920/1648877 [00:02<00:35, 44011.00it/s]\u001b[A\n  5%|▌         | 90112/1648877 [00:02<00:52, 29615.84it/s]\u001b[A\n  6%|▋         | 106496/1648877 [00:03<00:43, 35431.13it/s]\u001b[A\n  7%|▋         | 122880/1648877 [00:03<00:38, 40012.43it/s]\u001b[A\n  8%|▊         | 131072/1648877 [00:03<00:39, 38428.30it/s]\u001b[A\n  9%|▉         | 147456/1648877 [00:03<00:34, 43148.74it/s]\u001b[A\n 10%|▉         | 163840/1648877 [00:04<00:30, 47950.62it/s]\u001b[A\n 10%|█         | 172032/1648877 [00:04<00:33, 43586.53it/s]\u001b[A\n 11%|█▏        | 188416/1648877 [00:04<00:37, 38862.93it/s]\u001b[A\n 13%|█▎        | 212992/1648877 [00:05<00:31, 45377.79it/s]\u001b[A\n 13%|█▎        | 221184/1648877 [00:05<00:30, 46065.22it/s]\u001b[A\n 14%|█▍        | 229376/1648877 [00:05<00:41, 33968.09it/s]\u001b[A\n 15%|█▍        | 245760/1648877 [00:06<00:46, 30045.03it/s]\u001b[A\n 15%|█▌        | 253952/1648877 [00:06<01:00, 23139.03it/s]\u001b[A\n 16%|█▋        | 270336/1648877 [00:07<00:53, 25879.14it/s]\u001b[A\n 17%|█▋        | 278528/1648877 [00:07<00:49, 27598.82it/s]\u001b[A\n 17%|█▋        | 286720/1648877 [00:07<00:47, 28955.29it/s]\u001b[A\n 18%|█▊        | 294912/1648877 [00:08<00:45, 29983.12it/s]\u001b[A\n 18%|█▊        | 303104/1648877 [00:08<00:40, 33315.07it/s]\u001b[A\n 19%|█▉        | 311296/1648877 [00:08<00:34, 39085.95it/s]\u001b[A\n 19%|█▉        | 319488/1648877 [00:08<00:36, 36927.23it/s]\u001b[A\n 20%|█▉        | 327680/1648877 [00:08<00:36, 35877.14it/s]\u001b[A\n 20%|██        | 335872/1648877 [00:09<00:47, 27598.67it/s]\u001b[A\n 21%|██        | 344064/1648877 [00:09<00:45, 28954.28it/s]\u001b[A\n 21%|██▏       | 352256/1648877 [00:09<00:45, 28448.04it/s]\u001b[A\n 22%|██▏       | 360448/1648877 [00:10<00:43, 29604.10it/s]\u001b[A\n 22%|██▏       | 368640/1648877 [00:10<00:42, 30470.03it/s]\u001b[A\n 23%|██▎       | 376832/1648877 [00:10<00:34, 37326.93it/s]\u001b[A\n 23%|██▎       | 385024/1648877 [00:10<00:33, 37639.31it/s]\u001b[A\n 24%|██▍       | 393216/1648877 [00:11<00:34, 35955.90it/s]\u001b[A\n 24%|██▍       | 401408/1648877 [00:11<00:33, 36899.38it/s]\u001b[A\n 25%|██▍       | 409600/1648877 [00:11<00:34, 35522.49it/s]\u001b[A\n 25%|██▌       | 417792/1648877 [00:11<00:41, 29698.85it/s]\u001b[A\n 26%|██▌       | 425984/1648877 [00:12<00:34, 35782.89it/s]\u001b[A\n 26%|██▋       | 434176/1648877 [00:12<00:37, 32557.77it/s]\u001b[A\n 27%|██▋       | 442368/1648877 [00:12<00:37, 32538.05it/s]\u001b[A\n 27%|██▋       | 450560/1648877 [00:12<00:34, 34902.67it/s]\u001b[A\n 28%|██▊       | 458752/1648877 [00:12<00:31, 37760.10it/s]\u001b[A\n 28%|██▊       | 466944/1648877 [00:13<00:30, 38219.91it/s]\u001b[A\n 29%|██▉       | 475136/1648877 [00:13<00:37, 31406.37it/s]\u001b[A\n 29%|██▉       | 483328/1648877 [00:14<00:49, 23621.40it/s]\u001b[A\n 30%|██▉       | 491520/1648877 [00:14<00:41, 28142.74it/s]\u001b[A\n 30%|███       | 499712/1648877 [00:14<00:46, 24715.24it/s]\u001b[A\n9920512it [09:40, 22184.95it/s]\n 31%|███▏      | 516096/1648877 [00:15<01:04, 17565.14it/s]\u001b[A\n 32%|███▏      | 524288/1648877 [00:16<01:05, 17181.32it/s]\u001b[A\n 32%|███▏      | 532480/1648877 [00:16<00:55, 20009.30it/s]\u001b[A\n 33%|███▎      | 540672/1648877 [00:16<00:49, 22609.33it/s]\u001b[A\n 33%|███▎      | 548864/1648877 [00:17<00:44, 24912.43it/s]\u001b[A\n 34%|███▍      | 557056/1648877 [00:17<00:50, 21566.31it/s]\u001b[A\n 34%|███▍      | 565248/1648877 [00:17<00:45, 23993.47it/s]\u001b[A\n 35%|███▍      | 573440/1648877 [00:18<00:41, 26019.05it/s]\u001b[A\n 36%|███▌      | 589824/1648877 [00:18<00:35, 30079.85it/s]\u001b[A\n 36%|███▋      | 598016/1648877 [00:18<00:34, 30813.99it/s]\u001b[A\n 37%|███▋      | 606208/1648877 [00:18<00:29, 35051.55it/s]\u001b[A\n 37%|███▋      | 614400/1648877 [00:19<00:30, 34307.32it/s]\u001b[A\n 38%|███▊      | 622592/1648877 [00:19<00:26, 38794.32it/s]\u001b[A\n 38%|███▊      | 630784/1648877 [00:19<00:22, 45561.44it/s]\u001b[A\n 39%|███▉      | 638976/1648877 [00:19<00:34, 29650.90it/s]\u001b[A\n 40%|████      | 663552/1648877 [00:20<00:27, 35998.09it/s]\u001b[A\n 41%|████      | 671744/1648877 [00:20<00:39, 25030.90it/s]\u001b[A\n 42%|████▏     | 688128/1648877 [00:20<00:31, 30295.88it/s]\u001b[A\n 42%|████▏     | 696320/1648877 [00:21<00:30, 30927.22it/s]\u001b[A\n 43%|████▎     | 704512/1648877 [00:21<00:30, 31427.58it/s]\u001b[A\n 43%|████▎     | 712704/1648877 [00:21<00:29, 31778.72it/s]\u001b[A\n 44%|████▎     | 720896/1648877 [00:21<00:28, 32048.69it/s]\u001b[A\n 44%|████▍     | 729088/1648877 [00:22<00:41, 22026.98it/s]\u001b[A\n 45%|████▌     | 745472/1648877 [00:23<00:35, 25513.08it/s]\u001b[A\n 46%|████▌     | 753664/1648877 [00:23<00:32, 27270.01it/s]\u001b[A\n 46%|████▌     | 761856/1648877 [00:23<00:30, 28656.20it/s]\u001b[A\n 47%|████▋     | 770048/1648877 [00:24<00:37, 23351.48it/s]\u001b[A\n 48%|████▊     | 786432/1648877 [00:24<00:30, 28166.52it/s]\u001b[A\n 48%|████▊     | 794624/1648877 [00:24<00:31, 27464.66it/s]\u001b[A\n 49%|████▊     | 802816/1648877 [00:24<00:29, 28798.71it/s]\u001b[A\n 49%|████▉     | 811008/1648877 [00:25<00:26, 32206.35it/s]\u001b[A\n 50%|████▉     | 819200/1648877 [00:25<00:25, 32354.32it/s]\u001b[A\n 50%|█████     | 827392/1648877 [00:25<00:23, 34494.16it/s]\u001b[A\n 51%|█████     | 835584/1648877 [00:25<00:23, 34858.83it/s]\u001b[A\n 51%|█████     | 843776/1648877 [00:25<00:20, 39298.46it/s]\u001b[A\n 52%|█████▏    | 851968/1648877 [00:26<00:28, 27655.29it/s]\u001b[A\n 52%|█████▏    | 860160/1648877 [00:26<00:23, 33216.35it/s]\u001b[A\n 53%|█████▎    | 868352/1648877 [00:26<00:26, 29686.38it/s]\u001b[A\n 53%|█████▎    | 876544/1648877 [00:27<00:25, 30528.98it/s]\u001b[A\n 54%|█████▎    | 884736/1648877 [00:27<00:33, 22738.08it/s]\u001b[A\n 54%|█████▍    | 892928/1648877 [00:28<00:43, 17495.63it/s]\u001b[A\n 55%|█████▌    | 909312/1648877 [00:28<00:32, 22423.21it/s]\u001b[A\n 56%|█████▌    | 917504/1648877 [00:29<00:38, 19090.07it/s]\u001b[A\n 56%|█████▌    | 925696/1648877 [00:30<00:53, 13624.60it/s]\u001b[A\n 57%|█████▋    | 933888/1648877 [00:30<00:49, 14339.06it/s]\u001b[A\n 57%|█████▋    | 942080/1648877 [00:31<00:47, 14878.24it/s]\u001b[A\n 58%|█████▊    | 950272/1648877 [00:32<00:53, 12943.43it/s]\u001b[A\n 58%|█████▊    | 958464/1648877 [00:32<00:49, 14021.07it/s]\u001b[A\n 59%|█████▊    | 966656/1648877 [00:33<00:46, 14635.10it/s]\u001b[A\n 59%|█████▉    | 974848/1648877 [00:33<00:52, 12794.86it/s]\u001b[A\n 60%|█████▉    | 983040/1648877 [00:34<01:01, 10748.73it/s]\u001b[A\n 60%|██████    | 991232/1648877 [00:35<01:02, 10492.90it/s]\u001b[A\n 61%|██████    | 999424/1648877 [00:36<01:04, 10132.58it/s]\u001b[A\n 61%|██████    | 1007616/1648877 [00:36<00:50, 12767.52it/s]\u001b[A\n 62%|██████▏   | 1015808/1648877 [00:37<00:46, 13674.53it/s]\u001b[A\n 62%|██████▏   | 1024000/1648877 [00:37<00:37, 16543.93it/s]\u001b[A\n 63%|██████▎   | 1032192/1648877 [00:37<00:31, 19417.51it/s]\u001b[A\n 63%|██████▎   | 1040384/1648877 [00:38<00:27, 22005.67it/s]\u001b[A\n 64%|██████▎   | 1048576/1648877 [00:38<00:25, 23321.12it/s]\u001b[A\n 64%|██████▍   | 1056768/1648877 [00:38<00:28, 20637.83it/s]\u001b[A\n 65%|██████▍   | 1064960/1648877 [00:39<00:25, 23332.62it/s]\u001b[A\n 65%|██████▌   | 1073152/1648877 [00:39<00:22, 25523.00it/s]\u001b[A\n 66%|██████▌   | 1081344/1648877 [00:39<00:20, 27321.36it/s]\u001b[A\n 67%|██████▋   | 1097728/1648877 [00:40<00:17, 31937.29it/s]\u001b[A\n 67%|██████▋   | 1105920/1648877 [00:40<00:15, 34377.52it/s]\u001b[A\n 68%|██████▊   | 1114112/1648877 [00:40<00:18, 29020.32it/s]\u001b[A\n 69%|██████▊   | 1130496/1648877 [00:40<00:14, 35616.74it/s]\u001b[A\n 69%|██████▉   | 1138688/1648877 [00:41<00:14, 34414.67it/s]\u001b[A\n 70%|██████▉   | 1146880/1648877 [00:41<00:17, 28607.91it/s]\u001b[A\n 71%|███████   | 1163264/1648877 [00:41<00:16, 29667.49it/s]\u001b[A\n 71%|███████   | 1171456/1648877 [00:42<00:15, 30472.15it/s]\u001b[A\n 72%|███████▏  | 1179648/1648877 [00:42<00:15, 31056.52it/s]\u001b[A\n 72%|███████▏  | 1187840/1648877 [00:42<00:14, 31366.94it/s]\u001b[A\n 73%|███████▎  | 1196032/1648877 [00:43<00:18, 24547.08it/s]\u001b[A\n 74%|███████▎  | 1212416/1648877 [00:43<00:18, 23651.88it/s]\u001b[A\n 74%|███████▍  | 1220608/1648877 [00:44<00:14, 29158.47it/s]\u001b[A\n 75%|███████▍  | 1228800/1648877 [00:44<00:17, 23609.28it/s]\u001b[A\n 75%|███████▌  | 1236992/1648877 [00:45<00:19, 20835.11it/s]\u001b[A\n 76%|███████▌  | 1245184/1648877 [00:45<00:18, 21941.77it/s]\u001b[A\n 76%|███████▌  | 1253376/1648877 [00:45<00:16, 24309.49it/s]\u001b[A\n 77%|███████▋  | 1261568/1648877 [00:46<00:18, 21181.50it/s]\u001b[A\n 77%|███████▋  | 1269760/1648877 [00:46<00:18, 20569.35it/s]\u001b[A\n 78%|███████▊  | 1277952/1648877 [00:47<00:19, 19085.30it/s]\u001b[A\n 78%|███████▊  | 1286144/1648877 [00:48<00:28, 12659.04it/s]\u001b[A\n 78%|███████▊  | 1294336/1648877 [00:48<00:28, 12644.86it/s]\u001b[A\n 79%|███████▉  | 1302528/1648877 [00:49<00:25, 13548.07it/s]\u001b[A\n 79%|███████▉  | 1310720/1648877 [00:49<00:23, 14267.67it/s]\u001b[A\n 80%|███████▉  | 1318912/1648877 [00:50<00:22, 14825.57it/s]\u001b[A\n 80%|████████  | 1327104/1648877 [00:50<00:21, 15250.81it/s]\u001b[A\n 81%|████████  | 1335296/1648877 [00:51<00:21, 14931.56it/s]\u001b[A\n 81%|████████▏ | 1343488/1648877 [00:52<00:19, 15330.62it/s]\u001b[A\n 82%|████████▏ | 1351680/1648877 [00:52<00:19, 15622.53it/s]\u001b[A\n 82%|████████▏ | 1359872/1648877 [00:52<00:16, 17951.21it/s]\u001b[A\n 83%|████████▎ | 1368064/1648877 [00:53<00:15, 17936.47it/s]\u001b[A\n 83%|████████▎ | 1376256/1648877 [00:53<00:13, 20724.31it/s]\u001b[A\n 84%|████████▍ | 1384448/1648877 [00:53<00:11, 22361.13it/s]\u001b[A\n 84%|████████▍ | 1392640/1648877 [00:54<00:10, 24673.78it/s]\u001b[A\n 85%|████████▍ | 1400832/1648877 [00:54<00:11, 21380.84it/s]\u001b[A\n 85%|████████▌ | 1409024/1648877 [00:54<00:09, 24043.87it/s]\u001b[A\n 86%|████████▌ | 1417216/1648877 [00:55<00:08, 25860.18it/s]\u001b[A\n 86%|████████▋ | 1425408/1648877 [00:55<00:08, 26355.10it/s]\u001b[A\n 87%|████████▋ | 1433600/1648877 [00:55<00:07, 27932.80it/s]\u001b[A\n 87%|████████▋ | 1441792/1648877 [00:55<00:06, 30773.05it/s]\u001b[A\n 88%|████████▊ | 1449984/1648877 [00:56<00:06, 31317.06it/s]\u001b[A\n 88%|████████▊ | 1458176/1648877 [00:56<00:05, 36093.71it/s]\u001b[A\n 89%|████████▉ | 1466368/1648877 [00:56<00:04, 37055.23it/s]\u001b[A\n 89%|████████▉ | 1474560/1648877 [00:56<00:04, 35624.98it/s]\u001b[A\n 90%|████████▉ | 1482752/1648877 [00:57<00:06, 26277.34it/s]\u001b[A\n 90%|█████████ | 1490944/1648877 [00:57<00:05, 27185.09it/s]\u001b[A\n 91%|█████████ | 1499136/1648877 [00:57<00:05, 25413.65it/s]\u001b[A\n 91%|█████████▏| 1507328/1648877 [00:58<00:05, 24662.46it/s]\u001b[A\n 92%|█████████▏| 1515520/1648877 [00:58<00:06, 20764.87it/s]\u001b[A\n 92%|█████████▏| 1523712/1648877 [00:59<00:05, 21357.22it/s]\u001b[A\n 93%|█████████▎| 1531904/1648877 [00:59<00:05, 21185.07it/s]\u001b[A\n 93%|█████████▎| 1540096/1648877 [00:59<00:04, 23644.41it/s]\u001b[A\n 94%|█████████▍| 1548288/1648877 [01:00<00:05, 19708.51it/s]\u001b[A\n 95%|█████████▍| 1564672/1648877 [01:01<00:04, 20343.29it/s]\u001b[A\n 95%|█████████▌| 1572864/1648877 [01:01<00:03, 25208.42it/s]\u001b[A\n 96%|█████████▌| 1581056/1648877 [01:01<00:03, 21683.51it/s]\u001b[A\n 96%|█████████▋| 1589248/1648877 [01:02<00:02, 20570.76it/s]\u001b[A\n 97%|█████████▋| 1597440/1648877 [01:02<00:02, 22051.80it/s]\u001b[A\n 97%|█████████▋| 1605632/1648877 [01:02<00:02, 19941.91it/s]\u001b[A\n 98%|█████████▊| 1613824/1648877 [01:03<00:01, 22902.88it/s]\u001b[A\n 98%|█████████▊| 1622016/1648877 [01:03<00:01, 24701.82it/s]\u001b[A\n 99%|█████████▉| 1630208/1648877 [01:03<00:00, 25470.78it/s]\u001b[A\n 99%|█████████▉| 1638400/1648877 [01:04<00:00, 27244.18it/s]\u001b[A\n100%|█████████▉| 1646592/1648877 [01:04<00:00, 24072.31it/s]\u001b[A\n\n0it [00:00, ?it/s]\u001b[A\u001b[AExtracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n  0%|          | 0/4542 [00:00<?, ?it/s]\u001b[A\u001b[A\n\n8192it [00:00, 25237.79it/s]\u001b[A\u001b[AExtracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\nProcessing...\nDone!\n"
    }
   ],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1,28x28\n",
    "        self.conv1= nn.Conv2d(1,10,5) # 10, 24x24\n",
    "        self.conv2=nn.Conv2d(10,20,3) # 128, 10x10\n",
    "        self.fc1 = nn.Linear(20*10*10,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0)\n",
    "        \n",
    "        out = self.conv1(x) #24\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)  #12\n",
    "        \n",
    "        out = self.conv2(out) #10\n",
    "        out = F.relu(out)\n",
    "        out = out.view(in_size,-1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "model = SimpleNet()\n",
    "# TODO:define loss function and optimiter\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train Epoch: 1 [3712/60000 (6%)]\tLoss: 0.002428\nTrain Epoch: 1 [7552/60000 (13%)]\tLoss: 0.022574\nTrain Epoch: 1 [11392/60000 (19%)]\tLoss: 0.001869\nTrain Epoch: 1 [15232/60000 (25%)]\tLoss: 0.000149\nTrain Epoch: 1 [19072/60000 (32%)]\tLoss: 0.000577\nTrain Epoch: 1 [22912/60000 (38%)]\tLoss: 0.001686\nTrain Epoch: 1 [26752/60000 (45%)]\tLoss: 0.000652\nTrain Epoch: 1 [30592/60000 (51%)]\tLoss: 0.000157\nTrain Epoch: 1 [34432/60000 (57%)]\tLoss: 0.000412\nTrain Epoch: 1 [38272/60000 (64%)]\tLoss: 0.000671\nTrain Epoch: 1 [42112/60000 (70%)]\tLoss: 0.002315\nTrain Epoch: 1 [45952/60000 (77%)]\tLoss: 0.000966\nTrain Epoch: 1 [49792/60000 (83%)]\tLoss: 0.000071\nTrain Epoch: 1 [53632/60000 (90%)]\tLoss: 0.002035\nTrain Epoch: 1 [57472/60000 (96%)]\tLoss: 0.000264\n\nTraining set: Average loss: 0.0323, Accuracy: 59790/60000 (100%)\n\n\nTest set: Average loss: 0.0386, Accuracy: 9881/10000 (99%)\n\nTrain Epoch: 2 [3712/60000 (6%)]\tLoss: 0.002242\nTrain Epoch: 2 [7552/60000 (13%)]\tLoss: 0.040883\nTrain Epoch: 2 [11392/60000 (19%)]\tLoss: 0.007677\nTrain Epoch: 2 [15232/60000 (25%)]\tLoss: 0.012118\nTrain Epoch: 2 [19072/60000 (32%)]\tLoss: 0.032352\nTrain Epoch: 2 [22912/60000 (38%)]\tLoss: 0.000171\nTrain Epoch: 2 [26752/60000 (45%)]\tLoss: 0.002138\nTrain Epoch: 2 [30592/60000 (51%)]\tLoss: 0.034548\nTrain Epoch: 2 [34432/60000 (57%)]\tLoss: 0.008894\nTrain Epoch: 2 [38272/60000 (64%)]\tLoss: 0.003093\nTrain Epoch: 2 [42112/60000 (70%)]\tLoss: 0.002188\nTrain Epoch: 2 [45952/60000 (77%)]\tLoss: 0.000050\nTrain Epoch: 2 [49792/60000 (83%)]\tLoss: 0.008357\nTrain Epoch: 2 [53632/60000 (90%)]\tLoss: 0.006176\nTrain Epoch: 2 [57472/60000 (96%)]\tLoss: 0.013053\n\nTraining set: Average loss: 0.0306, Accuracy: 59814/60000 (100%)\n\n\nTest set: Average loss: 0.0418, Accuracy: 9887/10000 (99%)\n\nTrain Epoch: 3 [3712/60000 (6%)]\tLoss: 0.031224\nTrain Epoch: 3 [7552/60000 (13%)]\tLoss: 0.000315\nTrain Epoch: 3 [11392/60000 (19%)]\tLoss: 0.001193\nTrain Epoch: 3 [15232/60000 (25%)]\tLoss: 0.002473\nTrain Epoch: 3 [19072/60000 (32%)]\tLoss: 0.000103\nTrain Epoch: 3 [22912/60000 (38%)]\tLoss: 0.000284\nTrain Epoch: 3 [26752/60000 (45%)]\tLoss: 0.000110\nTrain Epoch: 3 [30592/60000 (51%)]\tLoss: 0.001080\nTrain Epoch: 3 [34432/60000 (57%)]\tLoss: 0.000914\nTrain Epoch: 3 [38272/60000 (64%)]\tLoss: 0.019619\nTrain Epoch: 3 [42112/60000 (70%)]\tLoss: 0.001863\nTrain Epoch: 3 [45952/60000 (77%)]\tLoss: 0.000018\nTrain Epoch: 3 [49792/60000 (83%)]\tLoss: 0.007671\nTrain Epoch: 3 [53632/60000 (90%)]\tLoss: 0.024753\nTrain Epoch: 3 [57472/60000 (96%)]\tLoss: 0.010261\n\nTraining set: Average loss: 0.0274, Accuracy: 59818/60000 (100%)\n\n\nTest set: Average loss: 0.0416, Accuracy: 9893/10000 (99%)\n\nTrain Epoch: 4 [3712/60000 (6%)]\tLoss: 0.000873\nTrain Epoch: 4 [7552/60000 (13%)]\tLoss: 0.000124\nTrain Epoch: 4 [11392/60000 (19%)]\tLoss: 0.028969\nTrain Epoch: 4 [15232/60000 (25%)]\tLoss: 0.000741\nTrain Epoch: 4 [19072/60000 (32%)]\tLoss: 0.006051\nTrain Epoch: 4 [22912/60000 (38%)]\tLoss: 0.010520\nTrain Epoch: 4 [26752/60000 (45%)]\tLoss: 0.006952\nTrain Epoch: 4 [30592/60000 (51%)]\tLoss: 0.002850\nTrain Epoch: 4 [34432/60000 (57%)]\tLoss: 0.008472\nTrain Epoch: 4 [38272/60000 (64%)]\tLoss: 0.000422\nTrain Epoch: 4 [42112/60000 (70%)]\tLoss: 0.009260\nTrain Epoch: 4 [45952/60000 (77%)]\tLoss: 0.037774\nTrain Epoch: 4 [49792/60000 (83%)]\tLoss: 0.000236\nTrain Epoch: 4 [53632/60000 (90%)]\tLoss: 0.000319\nTrain Epoch: 4 [57472/60000 (96%)]\tLoss: 0.000357\n\nTraining set: Average loss: 0.0134, Accuracy: 59856/60000 (100%)\n\n\nTest set: Average loss: 0.0378, Accuracy: 9892/10000 (99%)\n\nTrain Epoch: 5 [3712/60000 (6%)]\tLoss: 0.000086\nTrain Epoch: 5 [7552/60000 (13%)]\tLoss: 0.000460\nTrain Epoch: 5 [11392/60000 (19%)]\tLoss: 0.000570\nTrain Epoch: 5 [15232/60000 (25%)]\tLoss: 0.005842\nTrain Epoch: 5 [19072/60000 (32%)]\tLoss: 0.012325\nTrain Epoch: 5 [22912/60000 (38%)]\tLoss: 0.000116\nTrain Epoch: 5 [26752/60000 (45%)]\tLoss: 0.000065\nTrain Epoch: 5 [30592/60000 (51%)]\tLoss: 0.000002\nTrain Epoch: 5 [34432/60000 (57%)]\tLoss: 0.000139\nTrain Epoch: 5 [38272/60000 (64%)]\tLoss: 0.000309\nTrain Epoch: 5 [42112/60000 (70%)]\tLoss: 0.000168\nTrain Epoch: 5 [45952/60000 (77%)]\tLoss: 0.000385\nTrain Epoch: 5 [49792/60000 (83%)]\tLoss: 0.003363\nTrain Epoch: 5 [53632/60000 (90%)]\tLoss: 0.000067\nTrain Epoch: 5 [57472/60000 (96%)]\tLoss: 0.000541\n\nTraining set: Average loss: 0.0058, Accuracy: 59883/60000 (100%)\n\n\nTest set: Average loss: 0.0387, Accuracy: 9903/10000 (99%)\n\nTrain Epoch: 6 [3712/60000 (6%)]\tLoss: 0.000133\nTrain Epoch: 6 [7552/60000 (13%)]\tLoss: 0.000037\nTrain Epoch: 6 [11392/60000 (19%)]\tLoss: 0.000057\nTrain Epoch: 6 [15232/60000 (25%)]\tLoss: 0.000261\nTrain Epoch: 6 [19072/60000 (32%)]\tLoss: 0.000013\nTrain Epoch: 6 [22912/60000 (38%)]\tLoss: 0.000011\nTrain Epoch: 6 [26752/60000 (45%)]\tLoss: 0.000034\nTrain Epoch: 6 [30592/60000 (51%)]\tLoss: 0.000342\nTrain Epoch: 6 [34432/60000 (57%)]\tLoss: 0.000147\nTrain Epoch: 6 [38272/60000 (64%)]\tLoss: 0.006575\nTrain Epoch: 6 [42112/60000 (70%)]\tLoss: 0.004464\nTrain Epoch: 6 [45952/60000 (77%)]\tLoss: 0.103586\nTrain Epoch: 6 [49792/60000 (83%)]\tLoss: 0.000152\nTrain Epoch: 6 [53632/60000 (90%)]\tLoss: 0.008691\nTrain Epoch: 6 [57472/60000 (96%)]\tLoss: 0.005558\n"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(DEVICE)\n",
    "# train and evaluate\n",
    "\n",
    "for epoch in range(1,1+NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "    #         print(output.shape, target.shape)\n",
    "    #         print(output.dtype, target.dtype)\n",
    "    #         print('\\n\\n')\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%30 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    # Evaluate the training error\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTraining set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(train_loader.dataset),\n",
    "            100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "    # evaluate\n",
    "    # TODO:calculate the accuracy using traning and testing dataset\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training Set Accuracy = 99.6% \n",
    "Testing Set Accuracy = 99.03%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}